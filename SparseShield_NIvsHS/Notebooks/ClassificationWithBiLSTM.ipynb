{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# thx: https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#deal with tensors\r\n",
    "import torch   \r\n",
    "import torch.nn as nn\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "#handling text data\r\n",
    "from torchtext import data, vocab\r\n",
    "\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import spacy\r\n",
    "\r\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Reproducing same results\r\n",
    "SEED = 2020\r\n",
    "N_EPOCHS = 50\r\n",
    "ds_path = ''\r\n",
    "output_path = ''\r\n",
    "\r\n",
    "#Torch\r\n",
    "torch.manual_seed(SEED)\r\n",
    "\r\n",
    "#Cuda algorithms\r\n",
    "torch.backends.cudnn.deterministic = True  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# python -m spacy download en\r\n",
    "\r\n",
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\r\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\r\n",
    "fields = [('text',TEXT),('label', LABEL)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#loading custom dataset\r\n",
    "training_data=data.TabularDataset(path = ds_path,format = 'csv',fields = fields,skip_header = True)\r\n",
    "\r\n",
    "#print preprocessed text\r\n",
    "print(vars(training_data.examples[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data, valid_data = training_data.split(split_ratio=0.8, random_state = random.seed(SEED))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fullListHere : https://torchtext.readthedocs.io/en/latest/vocab.html\r\n",
    "# glove = glove.6B.100d\r\n",
    "# fasttext = fasttext.en.300d\r\n",
    "#initialize embeddings\r\n",
    "vec = vocab.Vectors('glove.6B.100d')\r\n",
    "\r\n",
    "TEXT.build_vocab(train_data,min_freq=3,vectors=vec)\r\n",
    "LABEL.build_vocab(train_data)\r\n",
    "\r\n",
    "#No. of unique tokens in text\r\n",
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\r\n",
    "\r\n",
    "#No. of unique tokens in label\r\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\r\n",
    "\r\n",
    "#Commonly used words\r\n",
    "print(TEXT.vocab.freqs.most_common(10))  \r\n",
    "\r\n",
    "#Word dictionary\r\n",
    "# print(TEXT.vocab.stoi) "
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#check whether cuda is available\r\n",
    "train_device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "print('Will train using: ' + train_device)\r\n",
    "device = torch.device(train_device)  \r\n",
    "\r\n",
    "#set batch size\r\n",
    "BATCH_SIZE = 64\r\n",
    "\r\n",
    "#Load an iterator\r\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\r\n",
    "    (train_data, valid_data), \r\n",
    "    batch_size = BATCH_SIZE,\r\n",
    "    sort_key = lambda x: len(x.text),\r\n",
    "    sort_within_batch=True,\r\n",
    "    device = device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class classifier(nn.Module):\r\n",
    "    \r\n",
    "    #define all the layers used in model\r\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n",
    "                 bidirectional, dropout):\r\n",
    "        \r\n",
    "        #Constructor\r\n",
    "        super().__init__()          \r\n",
    "        \r\n",
    "        #embedding layer\r\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
    "        \r\n",
    "        #lstm layer\r\n",
    "        self.lstm = nn.LSTM(embedding_dim, \r\n",
    "                           hidden_dim, \r\n",
    "                           num_layers=n_layers, \r\n",
    "                           bidirectional=bidirectional, \r\n",
    "                           dropout=dropout,\r\n",
    "                           batch_first=True)\r\n",
    "        \r\n",
    "        #dense layer\r\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\r\n",
    "        \r\n",
    "        #activation function\r\n",
    "        self.act = nn.Sigmoid()\r\n",
    "        \r\n",
    "    def forward(self, text, text_lengths):\r\n",
    "        \r\n",
    "        #text = [batch size,sent_length]\r\n",
    "        embedded = self.embedding(text)\r\n",
    "        #embedded = [batch size, sent_len, emb dim]\r\n",
    "      \r\n",
    "        #packed sequence\r\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths,batch_first=True)\r\n",
    "        \r\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\r\n",
    "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
    "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
    "        \r\n",
    "        #concat the final forward and backward hidden state\r\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n",
    "                \r\n",
    "        #hidden = [batch size, hid dim * num directions]\r\n",
    "        dense_outputs=self.fc(hidden)\r\n",
    "\r\n",
    "        #Final activation function\r\n",
    "        outputs=self.act(dense_outputs)\r\n",
    "        \r\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#define hyperparameters\r\n",
    "size_of_vocab = len(TEXT.vocab)\r\n",
    "embedding_dim = 3\r\n",
    "num_hidden_nodes = 32\r\n",
    "num_output_nodes = 1\r\n",
    "num_layers = 2\r\n",
    "bidirection = True\r\n",
    "dropout = 0.2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#instantiate the model\r\n",
    "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \r\n",
    "                   bidirectional = True, dropout = dropout)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#architecture\r\n",
    "print(model)\r\n",
    "\r\n",
    "#No. of trianable parameters\r\n",
    "def count_parameters(model):\r\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
    "    \r\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\r\n",
    "\r\n",
    "#Initialize the pretrained embedding\r\n",
    "pretrained_embeddings = TEXT.vocab.vectors\r\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\r\n",
    "\r\n",
    "print(pretrained_embeddings.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#define optimizer and loss\r\n",
    "optimizer = optim.Adam(model.parameters())\r\n",
    "criterion = nn.BCELoss()\r\n",
    "\r\n",
    "#define metric\r\n",
    "def binary_accuracy(preds, y):\r\n",
    "    #round predictions to the closest integer\r\n",
    "    rounded_preds = torch.round(preds)\r\n",
    "    \r\n",
    "    correct = (rounded_preds == y).float() \r\n",
    "    acc = correct.sum() / len(correct)\r\n",
    "    return acc\r\n",
    "    \r\n",
    "#push to cuda if available\r\n",
    "model = model.to(device)\r\n",
    "criterion = criterion.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model, iterator, optimizer, criterion):\r\n",
    "    \r\n",
    "    #initialize every epoch \r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_acc = 0\r\n",
    "    epoch_f1 = 0\r\n",
    "    epoch_precision = 0\r\n",
    "    epoch_recall = 0\r\n",
    "    \r\n",
    "    #set the model in training phase\r\n",
    "    model.train()  \r\n",
    "    \r\n",
    "    for batch in iterator:\r\n",
    "        \r\n",
    "        #resets the gradients after every batch\r\n",
    "        optimizer.zero_grad()   \r\n",
    "        \r\n",
    "        #retrieve text and no. of words\r\n",
    "        text, text_lengths = batch.text   \r\n",
    "        \r\n",
    "        #convert to 1D tensor\r\n",
    "        predictions = model(text, text_lengths).squeeze()  \r\n",
    "        \r\n",
    "        #compute the loss\r\n",
    "        loss = criterion(predictions, batch.label)        \r\n",
    "        \r\n",
    "        #compute the binary accuracy\r\n",
    "        acc = binary_accuracy(predictions, batch.label)   \r\n",
    "\r\n",
    "        #round predictions to the closest integer\r\n",
    "        predicted = torch.round(predictions).tolist()\r\n",
    "        real = batch.label.tolist()\r\n",
    "\r\n",
    "        #compute the f1_score\r\n",
    "        f1score = f1_score(real, predicted, average=\"macro\")\r\n",
    "\r\n",
    "        #compute the precision\r\n",
    "        precision = precision_score(real, predicted, average=\"macro\")\r\n",
    "\r\n",
    "        #compute the recall\r\n",
    "        recall = recall_score(real, predicted, average=\"macro\")\r\n",
    "        \r\n",
    "        #backpropage the loss and compute the gradients\r\n",
    "        loss.backward()       \r\n",
    "        \r\n",
    "        #update the weights\r\n",
    "        optimizer.step()      \r\n",
    "        \r\n",
    "        #loss and accuracy\r\n",
    "        epoch_loss += loss.item()  \r\n",
    "        epoch_acc += acc.item()  \r\n",
    "        epoch_f1 += f1score\r\n",
    "        epoch_precision += precision\r\n",
    "        epoch_recall += recall  \r\n",
    "        \r\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(model, iterator, criterion):\r\n",
    "    \r\n",
    "    #initialize every epoch\r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_acc = 0\r\n",
    "    epoch_f1 = 0\r\n",
    "    epoch_precision = 0\r\n",
    "    epoch_recall = 0\r\n",
    "\r\n",
    "    #deactivating dropout layers\r\n",
    "    model.eval()\r\n",
    "    \r\n",
    "    #deactivates autograd\r\n",
    "    with torch.no_grad():\r\n",
    "    \r\n",
    "        for batch in iterator:\r\n",
    "        \r\n",
    "            #retrieve text and no. of words\r\n",
    "            text, text_lengths = batch.text\r\n",
    "            \r\n",
    "            #convert to 1d tensor\r\n",
    "            predictions = model(text, text_lengths).squeeze()\r\n",
    "            \r\n",
    "            #compute loss and accuracy\r\n",
    "            loss = criterion(predictions, batch.label)\r\n",
    "            acc = binary_accuracy(predictions, batch.label)\r\n",
    "\r\n",
    "            #round predictions to the closest integer\r\n",
    "            predicted = torch.round(predictions).tolist()\r\n",
    "            real = batch.label.tolist()\r\n",
    "\r\n",
    "            #compute the f1_score\r\n",
    "            f1score = f1_score(real, predicted, average=\"macro\")\r\n",
    "\r\n",
    "            #compute the precision\r\n",
    "            precision = precision_score(real, predicted, average=\"macro\")\r\n",
    "\r\n",
    "            #compute the recall\r\n",
    "            recall = recall_score(real, predicted, average=\"macro\")\r\n",
    "            \r\n",
    "            #keep track of loss and accuracy\r\n",
    "            epoch_loss += loss.item()  \r\n",
    "            epoch_acc += acc.item()  \r\n",
    "            epoch_f1 += f1score\r\n",
    "            epoch_precision += precision\r\n",
    "            epoch_recall += recall  \r\n",
    "        \r\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_f1 / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Print iterations progress\r\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\r\n",
    "    \"\"\"\r\n",
    "    Call in a loop to create terminal progress bar\r\n",
    "    @params:\r\n",
    "        iteration   - Required  : current iteration (Int)\r\n",
    "        total       - Required  : total iterations (Int)\r\n",
    "        prefix      - Optional  : prefix string (Str)\r\n",
    "        suffix      - Optional  : suffix string (Str)\r\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\r\n",
    "        length      - Optional  : character length of bar (Int)\r\n",
    "        fill        - Optional  : bar fill character (Str)\r\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\r\n",
    "    \"\"\"\r\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\r\n",
    "    filledLength = int(length * iteration // total)\r\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\r\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)\r\n",
    "    # Print New Line on Complete\r\n",
    "    if iteration == total: \r\n",
    "        print()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_valid_loss = float('inf')\r\n",
    "\r\n",
    "columns = 'Name,Epoch,Loss,Acc,F1,Precision,Recall'\r\n",
    "f = open(output_path,'w')\r\n",
    "f.write(columns)\r\n",
    "f.write('\\n')\r\n",
    "\r\n",
    "printProgressBar(0, N_EPOCHS, prefix = 'Progress:', suffix = 'Complete', length = N_EPOCHS)\r\n",
    "for epoch in range(N_EPOCHS):\r\n",
    "     \r\n",
    "    #train the model\r\n",
    "    train_loss, train_acc, train_f1, train_precision, train_recall = train(model, train_iterator, optimizer, criterion)\r\n",
    "    \r\n",
    "    #evaluate the model\r\n",
    "    valid_loss, valid_acc, valid_f1, valid_precision, valid_recall = evaluate(model, valid_iterator, criterion)\r\n",
    "    \r\n",
    "    #save the best model\r\n",
    "    if valid_loss < best_valid_loss:\r\n",
    "        best_valid_loss = valid_loss\r\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
    "\r\n",
    "    f.write(f'T-SNE_GLOVE_Train,{epoch},{train_loss*100:.2f},{train_acc*100:.2f},{train_f1*100:.2f},{train_precision*100:.2f},{train_recall*100:.2f}')\r\n",
    "    f.write('\\n')\r\n",
    "    f.write(f'T-SNE_GLOVE_Test,{epoch},{valid_loss*100:.2f},{valid_acc*100:.2f},{valid_f1*100:.2f},{valid_precision*100:.2f},{valid_recall*100:.2f}')\r\n",
    "    f.write('\\n')\r\n",
    "    \r\n",
    "    printProgressBar(epoch + 1, N_EPOCHS, prefix = 'Progress:', suffix = 'Complete', length = N_EPOCHS)\r\n",
    "\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}